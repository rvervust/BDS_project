{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, functions as F, Row\n",
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType, ArrayType\n",
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, RegexTokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('BDS').setMaster('spark://192.168.56.1:7077')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sql_sc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book review data\n",
    "When doing some research online, many datasets for English tweet sentiment analysis are available.\n",
    "However, Dutch datasets are not.\n",
    "\n",
    "Since labeling tweets manually will take too much time, we chose the next best alternative.\n",
    "A dataset containing book review data, each review is labeled as negative (0) or positive (1).\n",
    "We think this could be a suitable dataset to train or machine learning model on, because when you compare both the reviews and tweets, they are actually quite comparable.\n",
    "\n",
    "The dataset is ~74MB compressed and can be downloaded from here:\n",
    "**[110k Dutch Book Reviews Dataset](https://github.com/benjaminvdb/110kDBRD/releases/download/v2.0/110kDBRD_v2.tgz)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merged both test and train sets, because we will be using a random split later on for validation and testing\n",
    "REVIEWS_POSITIVE = './book_reviews/pos/'\n",
    "REVIEWS_NEGATIVE = './book_reviews/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Het Brussel-syndr...|    1|\n",
      "|Super om te lezen...|    1|\n",
      "|Dolf uit Amstelve...|    1|\n",
      "|Bijna iedereen wi...|    1|\n",
      "|De verzamelaar va...|    1|\n",
      "|Miss peregrine's ...|    1|\n",
      "|Jitzak, de roman ...|    1|\n",
      "|Was benieuwd naar...|    1|\n",
      "|Al voor verschijn...|    1|\n",
      "|Deze jongen kan s...|    1|\n",
      "|Dit is voor mij h...|    1|\n",
      "|Hoe verder ik in ...|    1|\n",
      "|In de nacht van 2...|    1|\n",
      "|Wat een serie! Ma...|    1|\n",
      "|Geeft een andere ...|    1|\n",
      "|Breek mijn hart n...|    1|\n",
      "|Wellicht komt het...|    1|\n",
      "|Een van de beste ...|    1|\n",
      "|Heerlijk luguber ...|    1|\n",
      "|Geloof je wat je ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataets and convert them to dataframes\n",
    "row = Row(\"text\")\n",
    "positive = sc.wholeTextFiles(REVIEWS_POSITIVE).map(lambda x: x[1]).map(row).toDF().withColumn('label', F.lit(1))\n",
    "negative = sc.wholeTextFiles(REVIEWS_NEGATIVE).map(lambda x: x[1]).map(row).toDF().withColumn('label', F.lit(0))\n",
    "book_data = positive.union(negative)\n",
    "book_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "We need to check if we have enough samples and if the labels are equally represented, we do this by using a countplot over the different labels in the datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fadc3ffc390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = book_data.rdd.map(lambda x: x[1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the spark pipeline and extract features\n",
    "To process the data we will be using Spark and its Machine Learning Library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn strings into arrays of words\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dutch stopwords\n",
    "stopw = stopwords.words('dutch')\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stopw)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now create a pipelined transformer\n",
    "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|                text|label|               words|            filtered|                  tf|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|Het Brussel-syndr...|    1|[het, brussel, sy...|[brussel, syndroo...|(28751,[0,2,7,10,...|\n",
      "|Super om te lezen...|    1|[super, om, te, l...|[super, lezen, le...|(28751,[3,6,229,9...|\n",
      "|Dolf uit Amstelve...|    1|[dolf, uit, amste...|[dolf, amstelveen...|(28751,[0,9,21,23...|\n",
      "|Bijna iedereen wi...|    1|[bijna, iedereen,...|[bijna, iedereen,...|(28751,[0,2,6,7,8...|\n",
      "|De verzamelaar va...|    1|[de, verzamelaar,...|[verzamelaar, joh...|(28751,[0,3,22,24...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we can make the transformation between the raw text and the counts\n",
    "cv_pipeline.transform(book_data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|label|               words|            filtered|                  tf|               tfidf|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|Het Brussel-syndr...|    1|[het, brussel, sy...|[brussel, syndroo...|(28751,[0,2,7,10,...|(28751,[0,2,7,10,...|\n",
      "|Super om te lezen...|    1|[super, om, te, l...|[super, lezen, le...|(28751,[3,6,229,9...|(28751,[3,6,229,9...|\n",
      "|Dolf uit Amstelve...|    1|[dolf, uit, amste...|[dolf, amstelveen...|(28751,[0,9,21,23...|(28751,[0,9,21,23...|\n",
      "|Bijna iedereen wi...|    1|[bijna, iedereen,...|[bijna, iedereen,...|(28751,[0,2,6,7,8...|(28751,[0,2,6,7,8...|\n",
      "|De verzamelaar va...|    1|[de, verzamelaar,...|[verzamelaar, joh...|(28751,[0,3,22,24...|(28751,[0,3,22,24...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf_pipeline.transform(book_data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = idf_pipeline.transform(book_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Sentiment\n",
    "To estimate the sentimate, we will be using a random split to create a train, validation and test set.\n",
    "The model we will be using is LogisticRegression, because it is robust, simple and performs quite well.\n",
    "\n",
    "Note: Loading the model from disk seems to be broken/bugged in recent releases of spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13412, 6597, 2243]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df, validation_df, testing_df = book_data.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "[training_df.count(), validation_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model and saving to disk\n"
     ]
    }
   ],
   "source": [
    "lambda_par = 0.02\n",
    "alpha_par = 0.3\n",
    "\n",
    "model_file = \"./sentiment_model_lr_2\"\n",
    "trained = False\n",
    "try: \n",
    "    lr = LogisticRegression.load(model_file)\n",
    "    print('Loaded pipeline model from disk')\n",
    "    trained = True\n",
    "except :\n",
    "    print('Creating model and saving to disk')\n",
    "    lr = LogisticRegression().\\\n",
    "        setLabelCol('label').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)\n",
    "    \n",
    "lr_pipeline = Pipeline(stages=[idf_pipeline, lr])\n",
    "if not trained:\n",
    "    lr_pipeline = lr_pipeline.fit(training_df)\n",
    "    lr_pipeline.stages[1].write().overwrite().save(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8631195998180992|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline.transform(validation_df).\\\n",
    "    select(F.expr('float(prediction = label)').alias('correct')).\\\n",
    "    select(F.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the model on twitter data\n",
    "We have gathered a lot of twitter data, there are several sources (the code for gathering the data can be found in the github repository):\n",
    "\n",
    "  - Twitter search API using tweepy \n",
    "  - Twitter stream API using tweepy\n",
    "  - Twitter history using getoldtweets3\n",
    "  \n",
    "For the report we will mainly focus on tweets gathered by the streaming API since the end of April and on historical tweets since the beginning of November 2019 to get a baseline.\n",
    "\n",
    "To filter tweets on Belgium, we used tweet keywords typical for Belgium and geotag information including:\n",
    "  - belgov\n",
    "  - Covid19Be\n",
    "  - Vakantie\n",
    "  - Exitplan\n",
    "  - Mondmaskers\n",
    "  - ...\n",
    "  \n",
    "Note: Our gathered tweets can be downloaded here: https://1drv.ms/u/s!AhH4j1tznDIOlZ8iJkMTi94B0RN_1g?e=3JQdMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_FOLDER = './tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders containing tweets to load\n",
    "folders = [x[0] for x in os.walk(TWEET_FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keyword|\n",
      "+--------------------+\n",
      "|./tweets\\blijfinu...|\n",
      "|./tweets\\exitstra...|\n",
      "|./tweets\\Covid19B...|\n",
      "|./tweets\\mondmask...|\n",
      "|./tweets\\vakantie...|\n",
      "|./tweets\\belgov_2...|\n",
      "|            ./tweets|\n",
      "|     ./tweets\\stream|\n",
      "+--------------------+\n",
      "\n",
      "3995600\n"
     ]
    }
   ],
   "source": [
    "# Load the tweets in a dataframe\n",
    "tweets = None\n",
    "for folder in folders:\n",
    "    df = sql_sc.read.option(\"header\", \"true\").csv(folders).withColumn('keyword', F.lit(folder))\n",
    "    if tweets is None:\n",
    "        tweets = df\n",
    "    else:\n",
    "        tweets = tweets.union(df)\n",
    "        \n",
    "tweets.select('keyword').distinct().show(20)\n",
    "print(tweets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995600\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "tweets.dropDuplicates(['tweet_id','keyword'])\n",
    "print(tweets.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify, we want to detect the language in the tweet and filter out all languages except dutch\n",
    "def get_lang(tweet):\n",
    "    try:\n",
    "        return detect(tweet)      \n",
    "    except LangDetectException as e:\n",
    "        return 'unknown'\n",
    "    except TypeError as e:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep dutch tweets\n",
    "udf_check_lang = udf(get_lang)\n",
    "tweets = tweets.withColumn('lang', udf_check_lang('text'))\n",
    "dutch_tweets = tweets[tweets.lang.isin(\"nl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predictions based on the trained model\n",
    "predicted_tweets = lr_pipeline.transform(dutch_tweets).select('text', 'prediction', 'created_at', 'keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+\n",
      "|keyword                                   |created_at|\n",
      "+------------------------------------------+----------+\n",
      "|tweets                                    |2020-05-02|\n",
      "|tweets\\belgov_2019-11-01_2020-05-16       |2020-01-20|\n",
      "|tweets\\belgov_2019-11-01_2020-05-16       |2020-02-13|\n",
      "|tweets\\blijfinuwkot_2019-11-01_2020-05-16 |2020-03-12|\n",
      "|tweets\\Covid19BE_2019-11-01_2020-05-16    |2020-03-25|\n",
      "|tweets\\exitstrategie_2019-11-01_2020-05-16|2020-03-10|\n",
      "|tweets\\exitstrategie_2019-11-01_2020-05-16|2019-11-02|\n",
      "|tweets\\exitstrategie_2019-11-01_2020-05-16|2019-11-14|\n",
      "|tweets\\mondmaskers_2019-11-01_2020-05-16  |2020-03-16|\n",
      "|tweets\\mondmaskers_2019-11-01_2020-05-16  |2020-04-20|\n",
      "|tweets\\stream                             |2020-01-02|\n",
      "|tweets                                    |2019-11-16|\n",
      "|tweets\\belgov_2019-11-01_2020-05-16       |2020-02-12|\n",
      "|tweets\\blijfinuwkot_2019-11-01_2020-05-16 |2020-05-07|\n",
      "|tweets\\blijfinuwkot_2019-11-01_2020-05-16 |2020-01-04|\n",
      "|tweets\\exitstrategie_2019-11-01_2020-05-16|2020-04-30|\n",
      "|tweets\\exitstrategie_2019-11-01_2020-05-16|2020-03-07|\n",
      "|tweets\\mondmaskers_2019-11-01_2020-05-16  |2020-02-23|\n",
      "|tweets\\stream                             |2020-05-02|\n",
      "|tweets                                    |2020-03-02|\n",
      "+------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_tweets_formatted = predicted_tweets.withColumn('created_at', F.substring('created_at', 0, 10))\n",
    "predicted_tweets_formatted = predicted_tweets_formatted.withColumn('keyword', F.regexp_extract('keyword','[\\/]([^\\/]+)$',1))\n",
    "predicted_tweets_formatted.select('keyword','created_at').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the raw data\n",
    "predicted_tweets_formatted.write.csv('predictions_2_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+-----+\n",
      "|created_at|             keyword|         prediction|count|\n",
      "+----------+--------------------+-------------------+-----+\n",
      "|2020-02-27|tweets\\belgov_201...| 0.3767258382642998|  507|\n",
      "|2020-04-17|tweets\\blijfinuwk...|0.36762360446570974| 1254|\n",
      "|2020-02-11|tweets\\blijfinuwk...| 0.4693396226415094|  424|\n",
      "|2020-03-23|tweets\\exitstrate...|0.32973362131029516| 1389|\n",
      "|2020-05-06|tweets\\mondmasker...|  0.333733223363575|26674|\n",
      "|2020-02-03|              tweets| 0.5012406947890818|  403|\n",
      "|2020-05-15|tweets\\belgov_201...| 0.3333333333333333| 7368|\n",
      "|2019-12-10|tweets\\blijfinuwk...| 0.4641638225255973|  293|\n",
      "|2020-05-10|tweets\\Covid19BE_...| 0.4525870997141555|24139|\n",
      "|2020-01-13|tweets\\exitstrate...| 0.5195402298850574|  435|\n",
      "|2020-04-14|       tweets\\stream|0.36656671664167917| 1334|\n",
      "|2020-04-17|tweets\\vakantie_2...| 0.3661858974358974| 1248|\n",
      "|2019-12-07|              tweets| 0.4581497797356828|  227|\n",
      "|2019-12-09|tweets\\belgov_201...|0.43346007604562736|  263|\n",
      "|2019-11-12|tweets\\blijfinuwk...| 0.4225352112676056|  284|\n",
      "|2020-03-18|       tweets\\stream| 0.3584474885844749| 1314|\n",
      "|2020-02-07|tweets\\vakantie_2...|0.43213296398891965|  361|\n",
      "|2019-12-28|              tweets| 0.4613526570048309|  414|\n",
      "|2020-02-15|              tweets| 0.4609756097560976|  410|\n",
      "|2020-02-17|              tweets| 0.4547677261613692|  409|\n",
      "+----------+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the data based on keyword and date\n",
    "grouped_tweeds_by_day = predicted_tweets_formatted.select('created_at','prediction','keyword')\\\n",
    "    .groupby('created_at','keyword').agg(F.avg('prediction').alias('prediction'),F.count('*').alias('count'))\n",
    "grouped_tweeds_by_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>keyword</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>tweets\\belgov_2019-11-01_2020-05-16</td>\n",
       "      <td>0.379447</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>tweets\\blijfinuwkot_2019-11-01_2020-05-16</td>\n",
       "      <td>0.368971</td>\n",
       "      <td>1244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>tweets\\blijfinuwkot_2019-11-01_2020-05-16</td>\n",
       "      <td>0.470309</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>tweets\\exitstrategie_2019-11-01_2020-05-16</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>tweets\\mondmaskers_2019-11-01_2020-05-16</td>\n",
       "      <td>0.333571</td>\n",
       "      <td>26690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>tweets</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>tweets\\belgov_2019-11-01_2020-05-16</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>7360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>tweets\\blijfinuwkot_2019-11-01_2020-05-16</td>\n",
       "      <td>0.460751</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>tweets\\Covid19BE_2019-11-01_2020-05-16</td>\n",
       "      <td>0.452327</td>\n",
       "      <td>24133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>tweets\\exitstrategie_2019-11-01_2020-05-16</td>\n",
       "      <td>0.519362</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_at                                     keyword  prediction  count\n",
       "0  2020-02-27         tweets\\belgov_2019-11-01_2020-05-16    0.379447    506\n",
       "1  2020-04-17   tweets\\blijfinuwkot_2019-11-01_2020-05-16    0.368971   1244\n",
       "2  2020-02-11   tweets\\blijfinuwkot_2019-11-01_2020-05-16    0.470309    421\n",
       "3  2020-03-23  tweets\\exitstrategie_2019-11-01_2020-05-16    0.327103   1391\n",
       "4  2020-05-06    tweets\\mondmaskers_2019-11-01_2020-05-16    0.333571  26690\n",
       "5  2020-02-03                                      tweets    0.500000    404\n",
       "6  2020-05-15         tweets\\belgov_2019-11-01_2020-05-16    0.332609   7360\n",
       "7  2019-12-10   tweets\\blijfinuwkot_2019-11-01_2020-05-16    0.460751    293\n",
       "8  2020-05-10      tweets\\Covid19BE_2019-11-01_2020-05-16    0.452327  24133\n",
       "9  2020-01-13  tweets\\exitstrategie_2019-11-01_2020-05-16    0.519362    439"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the aggregated data is much smaller, we can load it in memory and write it to a single file by using pandas\n",
    "grouped_tweeds_by_day_pd = grouped_tweeds_by_day.toPandas()\n",
    "grouped_tweeds_by_day_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_tweeds_by_day_pd.to_csv('predictions_aggredated_by_day_2_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualisation\n",
    "To visualize the data, we will be using PowerBi.\n",
    "Due to Ugent restrictions, we are not able to create a live page.\n",
    "However, the powerbi files will be available on github."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
